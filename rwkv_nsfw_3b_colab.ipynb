{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EquatorSavage/Ai-Learn/blob/master/rwkv_nsfw_3b_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNtCDuxs1fgT"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4XrTo4N1g7N"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install huggingface_hub\n",
        "!pip install pynvml\n",
        "!pip install rwkv\n",
        "!pip install Ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS5lqW1H1iwi"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os, gc, copy, torch\n",
        "from datetime import datetime\n",
        "from huggingface_hub import hf_hub_download\n",
        "from pynvml import *\n",
        "\n",
        "# Flag to check if GPU is present\n",
        "HAS_GPU = True\n",
        "\n",
        "# Model title and context size limit\n",
        "ctx_limit = 2000\n",
        "title = \"RWKV-5-H-World-3B\"\n",
        "model_file = \"rwkv-5-h-world-3B\"\n",
        "\n",
        "# Get the GPU count\n",
        "try:\n",
        "    nvmlInit()\n",
        "    GPU_COUNT = nvmlDeviceGetCount()\n",
        "    if GPU_COUNT > 0:\n",
        "        HAS_GPU = True\n",
        "        gpu_h = nvmlDeviceGetHandleByIndex(0)\n",
        "except NVMLError as error:\n",
        "    print(error)\n",
        "\n",
        "\n",
        "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
        "\n",
        "# Model strat to use\n",
        "MODEL_STRAT=\"cpu bf16\"\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\n",
        "\n",
        "# Switch to GPU mode\n",
        "if HAS_GPU == True :\n",
        "    os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
        "    MODEL_STRAT = \"cuda bf16\"\n",
        "\n",
        "# Load the model accordingly\n",
        "from rwkv.model import RWKV\n",
        "model_path = hf_hub_download(repo_id=\"a686d380/rwkv-5-h-world\", filename=f\"{model_file}.pth\")\n",
        "model = RWKV(model=model_path, strategy=MODEL_STRAT)\n",
        "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
        "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWob3QCr1lLW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prompt generation\n",
        "def generate_prompt(instruction, input=\"\"):\n",
        "    instruction = instruction.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "    input = input.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "    if input:\n",
        "        return f\"\"\"Instruction: {instruction}\n",
        "Input: {input}\n",
        "Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"User: hi\n",
        "Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\n",
        "User: {instruction}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Evaluation logic\n",
        "def evaluate(\n",
        "    ctx,\n",
        "    token_count=200,\n",
        "    temperature=1.0,\n",
        "    top_p=0.7,\n",
        "    presencePenalty = 0.1,\n",
        "    countPenalty = 0.1,\n",
        "):\n",
        "    print(ctx)\n",
        "    args = PIPELINE_ARGS(temperature = max(0.2, float(temperature)), top_p = float(top_p),\n",
        "                     alpha_frequency = countPenalty,\n",
        "                     alpha_presence = presencePenalty,\n",
        "                     token_ban = [], # ban the generation of some tokens\n",
        "                     token_stop = [0]) # stop generation whenever you see any token here\n",
        "    ctx = ctx.strip()\n",
        "    all_tokens = []\n",
        "    out_last = 0\n",
        "    out_str = ''\n",
        "    occurrence = {}\n",
        "    state = None\n",
        "    for i in range(int(token_count)):\n",
        "        out, state = model.forward(pipeline.encode(ctx)[-ctx_limit:] if i == 0 else [token], state)\n",
        "        for n in occurrence:\n",
        "            out[n] -= (args.alpha_presence + occurrence[n] * args.alpha_frequency)\n",
        "\n",
        "        token = pipeline.sample_logits(out, temperature=args.temperature, top_p=args.top_p)\n",
        "        if token in args.token_stop:\n",
        "            break\n",
        "        all_tokens += [token]\n",
        "        for xxx in occurrence:\n",
        "            occurrence[xxx] *= 0.996\n",
        "        if token not in occurrence:\n",
        "            occurrence[token] = 1\n",
        "        else:\n",
        "            occurrence[token] += 1\n",
        "\n",
        "        tmp = pipeline.decode(all_tokens[out_last:])\n",
        "        if '\\ufffd' not in tmp:\n",
        "            out_str += tmp\n",
        "            yield out_str.strip()\n",
        "            out_last = i + 1\n",
        "\n",
        "    if HAS_GPU == True :\n",
        "        gpu_info = nvmlDeviceGetMemoryInfo(gpu_h)\n",
        "        print(f'vram {gpu_info.total} used {gpu_info.used} free {gpu_info.free}')\n",
        "\n",
        "    del out\n",
        "    del state\n",
        "    gc.collect()\n",
        "\n",
        "    if HAS_GPU == True :\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    yield out_str.strip()\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "# Gradio blocks\n",
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.HTML(f\"<div style=\\\"text-align: center;\\\">\\n<h1>RWKV-5 World v2 - {title}</h1>\\n</div>\")\n",
        "    with gr.Tab(\"Raw Generation\"):\n",
        "        gr.Markdown(f\"This is RWKV-5-h \")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                prompt = gr.Textbox(lines=2, label=\"Prompt\", value=\"\"\"边儿上还有两条腿，修长、结实，光滑得出奇，潜伏着\n",
        "媚人的活力。他紧张得脊梁都皱了起来。但他不动声色\"\"\")\n",
        "                token_count = gr.Slider(10, 500, label=\"Max Tokens\", step=10, value=200)\n",
        "                temperature = gr.Slider(0.2, 2.0, label=\"Temperature\", step=0.1, value=1.0)\n",
        "                top_p = gr.Slider(0.0, 1.0, label=\"Top P\", step=0.05, value=0.3)\n",
        "                presence_penalty = gr.Slider(0.0, 1.0, label=\"Presence Penalty\", step=0.1, value=1)\n",
        "                count_penalty = gr.Slider(0.0, 1.0, label=\"Count Penalty\", step=0.1, value=1)\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    submit = gr.Button(\"Submit\", variant=\"primary\")\n",
        "                    clear = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "                output = gr.Textbox(label=\"Output\", lines=5)\n",
        "        data = gr.Dataset(components=[prompt, token_count, temperature, top_p, presence_penalty, count_penalty], label=\"Example Instructions\", headers=[\"Prompt\", \"Max Tokens\", \"Temperature\", \"Top P\", \"Presence Penalty\", \"Count Penalty\"])\n",
        "        submit.click(evaluate, [prompt, token_count, temperature, top_p, presence_penalty, count_penalty], [output])\n",
        "        clear.click(lambda: None, [], [output])\n",
        "        data.click(lambda x: x, [data], [prompt, token_count, temperature, top_p, presence_penalty, count_penalty])\n",
        "\n",
        "# Gradio launch\n",
        "demo.queue(max_size=10)\n",
        "demo.launch(share=True)\n",
        "print('''\n",
        "点击url\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIktMaxOetep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gb7D67-TeuC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}